# -*- coding: utf-8 -*-
"""Image Recognition with ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XPFJaqyxfL-ib_TxM8Z8mqmserJJKM5g

**Fashion Item Classification using ANN**

In the dynamic and fast-paced fashion industry, understanding and organizing product inventories efficiently is crucial for driving sales and enhancing customer experiences. One of the innovative applications of Artificial Intelligence in this sector is image-based classification, where machine learning models are trained to recognize and categorize fashion items based on visual data.

This project leverages the Fashion MNIST dataset, a well-known benchmark in computer vision and machine learning. The dataset was introduced by Zalando Research, and it serves as a modern replacement for the original MNIST handwritten digits dataset. Fashion MNIST consists of grayscale images (28x28 pixels) of 10 different types of clothing items, including T-shirts, trousers, sneakers, and more. Each image is labeled with a category ranging from 0 to 9

| Label | Article     |
| ----- | ----------- |
| 0     | T-shirt/top |
| 1     | Trouser     |
| 2     | Pullover    |
| 3     | Dress       |
| 4     | Coat        |
| 5     | Sandal      |
| 6     | Shirt       |
| 7     | Sneaker     |
| 8     | Bag         |
| 9     | Ankle boot  |

We will start by importing the datasets we found online. The dataset is already divided to train and test sets.
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
fashion_minst_test = pd.read_csv('fashion-mnist_test.csv')
fashion_minst_train = pd.read_csv('fashion-mnist_train.csv')

"""We will start by visualizing the first two images out of the train dataset in order to understand our data better."""

import matplotlib.pyplot as plt
import numpy as np

# Select the first two rows
for i in range(2):
    image = fashion_minst_train.iloc[i, 1:].values.reshape(28, 28)  # Get pixel values and reshape
    label = fashion_minst_train.iloc[i, 0]  # Get label

    plt.figure(figsize=(2, 2))
    plt.imshow(image, cmap='gray')
    plt.axis('off')
    plt.show()

"""Before starting working on our model, we will make some preprocessing operations in order to make sure that our data is ready to get an efficient model."""

# Separate features and labels
X_train = fashion_minst_train.drop("label", axis=1).values
y_train = fashion_minst_train["label"].values

X_test = fashion_minst_test.drop("label", axis=1).values
y_test = fashion_minst_test["label"].values

# Normalize the pixel values to [0, 1]
X_train = X_train / 255.0
X_test = X_test / 255.0

# Print dataset shapes for confirmation
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
print(X_test) #showcase the normalization operation

"""One last step before moving to our model building, I was curious about which items are the most existing in our train dataset."""

import seaborn as sns
plt.figure(figsize=(8, 5))
sns.countplot(x=y_train, palette="Set2")
plt.title("Class Distribution in Training Set")
plt.xlabel("Class Label (0–9)")
plt.ylabel("Number of Samples")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""We found out that there is equal distrubtion for all the items. Now we are going to move to our ANN model building."""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Build the ANN model
model = Sequential()

# Input layer (flattened 28x28 image -> 784 features)
model.add(Dense(128, activation='relu', input_dim=784))  # First hidden layer

# Additional hidden layer
model.add(Dense(128, activation='relu'))  # Second hidden layer

# Output layer (10 classes, softmax for multi-class classification)
model.add(Dense(10, activation='softmax'))

# Compile the model
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""This code builds an ANN with two hidden layers, each having 128 neurons and using ReLU activation. The output layer has 10 neurons for multi-class classification with softmax activation which is deticated for such classification. The model is compiled with SGD optimizer for weights optimization and sparse categorical cross-entropy loss function. The sparse categorical cross-entropy will compare the predicted probability for each class with the true class (integer) label and calculate the error. It’s trained for 25 epochs (the number of times the entire training dataset is passed through the neural network during the training process), and the accuracy is evaluated on the test set.

An accuracy of 88.95% means that our model correctly classified about 89% of the images in the test set.
The model is correctly predicting almost 9 out of 10 images in the test set.
The remaining 11.05% of the predictions are incorrect. This means that for approximately 1 in 9 images, the model is making a mistake.
"""

from sklearn.metrics import confusion_matrix

# Make predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Get the true classes
y_true = y_test

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Above is our confusion matrix. We can see that the model performed well in predicting the correct labels, but there are some misclassifications worth noting. For example, the model frequently confused item 6 (shirt) with trousers, pullover, and coat. Similarly, item 2 (pullover) was often misclassified as item 4 (coat) and item 6 (shirt). This indicates that the model struggles with distinguishing between certain classes that may share similar visual features. The quality of the images plays a significant role here, as similar shapes or patterns can cause confusion in classification.
Let's go more forward and visualize the miscalssified images.
"""

# Get the indices of the misclassified images
misclassified_idx = np.where(y_pred_classes != y_true)[0]

# Plot misclassified images
plt.figure(figsize=(12, 8))
for i, idx in enumerate(misclassified_idx[:9]):  # Display first 9 misclassified images
    plt.subplot(3, 3, i+1)
    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')
    plt.title(f"True: {y_true[idx]}, Pred: {y_pred_classes[idx]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

"""These are the first 9 items from the test set. For example, in the 2nd image, the model predicted a pullover as a shirt. Upon further inspection, if you look at image 8, you can see that the shirt's shape (which was predicted as a T-shirt) is very similar to the pullover. In fact, the shirt in image 8 looks quite a lot like a T-shirt, which suggests that the model is having trouble distinguishing between these two classes due to their similar shapes.

To conclude, our model performed well, but there is definitely room for improvement through hyperparameter tuning. Based on the visualizations, I believe the key to enhancing the model lies in either increasing the amount of training data, as ANN models typically require large datasets to make accurate predictions, or improving the quality of the images. We noticed that the model had significant confusion, particularly with shapes, which suggests that clearer, more distinct images could help the model differentiate better between similar classes.
"""